#!/usr/bin/env python3
"""
dali_random_read_numpy.py
-------------------------
Random-read benchmark using *.npy* shards generated by create_synthetic_numpy.py
"""

from __future__ import annotations
import argparse, bisect, glob, random, sys, time
from pathlib import Path
from typing import Sequence

import numpy as np
import torch
from nvidia.dali import fn, pipeline_def, types

try:    # DALI â‰¥ 1.30
    from nvidia.dali.plugin.pytorch import DALIGenericIterator, LastBatchPolicy
except ImportError:
    from nvidia.dali.plugin.base_iterator import DALIGenericIterator
    from nvidia.dali.plugin.pytorch import LastBatchPolicy


# ---------------------------------------------------------------------------
def open_shards(pattern: str):
    dirs = sorted(Path(p).parent for p in glob.glob(pattern))
    if not dirs:
        sys.exit(f"No matches for {pattern!r}")

    imgs, acts, sts, cum = [], [], [], [0]
    for d in dirs:
        imgs.append(np.load(d / "image.npy", mmap_mode="r"))
        acts.append(np.load(d / "actions.npy", mmap_mode="r"))
        sts.append(np.load(d / "state.npy", mmap_mode="r"))
        cum.append(cum[-1] + len(imgs[-1]))
    return imgs, acts, sts, cum


# ---------------------------------------------------------------------------
class ExternalIterator:
    def __init__(self, imgs, acts, sts, cum, global_idx, bs, rank, world):
        self.imgs, self.acts, self.sts, self.cum = imgs, acts, sts, cum
        start = len(global_idx) * rank // world
        end   = len(global_idx) * (rank + 1) // world
        self.idx = global_idx[start:end]
        self.bs  = bs
        self.n   = len(self.idx)
        self.i   = 0

    def __iter__(self): self.i = 0; return self

    def _fetch(self, g):
        s = bisect.bisect_right(self.cum, g) - 1
        loc = g - self.cum[s]
        return (self.imgs[s][loc],
                self.acts[s][loc],
                self.sts[s][loc])

    def __next__(self):
        if self.i >= self.n: raise StopIteration
        batch = [self._fetch(self.idx[(self.i+k) % self.n]) for k in range(self.bs)]
        self.i += self.bs
        return tuple(map(list, zip(*batch)))

    next = __next__


# ---------------------------------------------------------------------------
@pipeline_def
def pipe(eii, device_read_ahead: int):
    img, act, st = fn.external_source(
        source=eii, num_outputs=3, batch=True,
        dtype=[types.UINT8, types.FLOAT, types.FLOAT])
    return img, act, st


# ---------------------------------------------------------------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--shard", required=True,
                    help='glob "/mnt/weka/shards_numpy/*/image.npy"')
    ap.add_argument("--batch", type=int, default=256)
    ap.add_argument("--workers", type=int, default=4)
    ap.add_argument("--shuffle", action="store_true")
    ap.add_argument("--device-read-ahead", type=int, default=1)
    args = ap.parse_args()

    imgs, acts, sts, cum = open_shards(args.shard)
    total = cum[-1]
    print(f"ðŸ“¦ {len(imgs)} shards â†’ {total} samples")

    gidx = list(range(total))
    if args.shuffle: random.shuffle(gidx)

    n_gpu = max(1, min(1, torch.cuda.device_count()))
    pipes = []
    for dev in range(n_gpu):
        eii = ExternalIterator(imgs, acts, sts, cum, gidx,
                               args.batch, dev, n_gpu)
        pipes.append(pipe(batch_size=args.batch,
                          num_threads=args.workers,
                          device_id=dev,
                          eii=eii,
                          device_read_ahead=args.device_read_ahead))
    for p in pipes: p.build()

    it = DALIGenericIterator(
        pipes, ["image","actions","state"], size=total,
        last_batch_policy=LastBatchPolicy.PARTIAL, dynamic_shape=True)

    next(it)                           # warm-up
    t0 = time.perf_counter()

    for batch in next(it):             # `batch` is an OrderedDict
        for v in batch.values():
            # Newer wheels: DALI Tensor âžœ .as_cpu()
            if hasattr(v, "as_cpu"):
                v = v.as_cpu()
            # Older wheels: already a torch.Tensor
            if isinstance(v, torch.Tensor):
                v = v.cpu()            # sync to host
            if hasattr(v, "numpy"):    # final read to ensure timing includes copy
                _ = v.numpy()
        ms = (time.perf_counter() - t0)*1e3
    print(f"âœ… {args.batch*n_gpu} samples in {ms:.2f} ms "
          f"â†’ {ms/(args.batch*n_gpu):.3f} ms/sample")


if __name__ == "__main__":
    main()